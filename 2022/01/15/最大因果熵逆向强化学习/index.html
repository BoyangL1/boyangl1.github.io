<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.boyangl1.cn","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="最大因果熵逆向强化学习  马尔科夫决策过程 Markov decision process (MDP)   定义1： 马尔可夫决策过程定义为一个元组\(M &#x3D; \left( S,A,P_{T},R,P_{0},S_{\phi} \right)\) \(S\)：有限或无限状态集合 \(A\)：有限或无限的动作集合 \(P_{T}\)：状态转移概率分布，\(P_{T}(S_{j}|A">
<meta property="og:type" content="article">
<meta property="og:title" content="最大因果熵逆向强化学习">
<meta property="og:url" content="http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Boyang&#39;s blog">
<meta property="og:description" content="最大因果熵逆向强化学习  马尔科夫决策过程 Markov decision process (MDP)   定义1： 马尔可夫决策过程定义为一个元组\(M &#x3D; \left( S,A,P_{T},R,P_{0},S_{\phi} \right)\) \(S\)：有限或无限状态集合 \(A\)：有限或无限的动作集合 \(P_{T}\)：状态转移概率分布，\(P_{T}(S_{j}|A">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image1.png">
<meta property="og:image" content="http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image2.png">
<meta property="og:image" content="http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image3.png">
<meta property="og:image" content="http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image4.png">
<meta property="article:published_time" content="2022-01-15T05:26:28.000Z">
<meta property="article:modified_time" content="2022-05-18T03:15:40.586Z">
<meta property="article:author" content="李博洋">
<meta property="article:tag" content="逆向强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image1.png">


<link rel="canonical" href="http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/","path":"2022/01/15/最大因果熵逆向强化学习/","title":"最大因果熵逆向强化学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>最大因果熵逆向强化学习 | Boyang's blog</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Boyang's blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">分享生活，交流知识</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">17</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">17</span></a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="李博洋"
      src="/images/cat.png">
  <p class="site-author-name" itemprop="name">李博洋</p>
  <div class="site-description" itemprop="description">生命太短暂，不要去做一些根本没有人想要的东西。</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com//BoyangL1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;&#x2F;BoyangL1" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liboyang0209@gmail.com" title="E-Mail → mailto:liboyang0209@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.boyangl1.cn/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cat.png">
      <meta itemprop="name" content="李博洋">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Boyang's blog">
      <meta itemprop="description" content="生命太短暂，不要去做一些根本没有人想要的东西。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="最大因果熵逆向强化学习 | Boyang's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          最大因果熵逆向强化学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-15 13:26:28" itemprop="dateCreated datePublished" datetime="2022-01-15T13:26:28+08:00">2022-01-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>最大因果熵逆向强化学习</p>
<ol type="1">
<li><strong>马尔科夫决策过程 Markov decision process (MDP)</strong></li>
</ol>
<blockquote>
<p><strong>定义1：</strong> 马尔可夫决策过程定义为一个元组<span
class="math inline">\(M = \left( S,A,P_{T},R,P_{0},S_{\phi}
\right)\)</span></p>
<p><span class="math inline">\(S\)</span>：有限或无限状态集合</p>
<p><span class="math inline">\(A\)</span>：有限或无限的动作集合</p>
<p><span class="math inline">\(P_{T}\)</span>：状态转移概率分布，<span
class="math inline">\(P_{T}(S_{j}|A_{i},S_{i})\)</span>表示智能体在状态<span
class="math inline">\(S_{i}\)</span>采取动作<span
class="math inline">\(A_{i}\)</span>后转移到状态<span
class="math inline">\(S_{j}\)</span>的概率</p>
<p><span class="math inline">\(R\)</span>：<span
class="math inline">\(R(S,A)\)</span>表示立即奖励函数</p>
<p><span class="math inline">\(S_{\phi}\)</span>：最终状态集合
<span id="more"></span></p>
</blockquote>
<ol type="1">
<li><strong>强化学习 Reinforcement Learning（RL）</strong></li>
</ol>
<p>在RL中，其目标在于学习到一个策略<span
class="math inline">\(\pi\)</span>，从而指导智能体的MDP，其目的在于最大化累计奖励函数，为了用公式表达这个过程，我们需要定义状态值函数以及状态-动作值函数：</p>
<p><strong>定义2：</strong>
状态值函数，表示从某一状态开始，采用策略<span
class="math inline">\(\pi\)</span>获得的未来预期奖励。在离散且符合MDP的情况下，可以定义为以下迭代公式：</p>
<p><span class="math display">\[\begin{matrix}
V_{\pi}(S) = \sum_{A \in \mathcal{A}}^{}\left\lbrack \pi\left( A
\middle| S \right)\left\lbrack R(S,A) + \sum_{\begin{matrix}
S^{&#39;} \in S \\
\  \\
\end{matrix}}^{}{P_{T}\left( S^{&#39;} \middle| A,S \right)V_{\pi}\left(
S^{&#39;} \right)} \right\rbrack \right\rbrack \tag{1} \\
\end{matrix}\]</span></p>
<p><strong>定义3：</strong>
Q函数，也被称作动作-状态值函数,包含依据策略<span
class="math inline">\(\pi\)</span>在状态<span
class="math inline">\(s\)</span>采用动作<span
class="math inline">\(a\)</span>后获得的未来预期奖励。在离散且符合MDP的情况下，可以定义为以下公式：</p>
<p><span class="math display">\[\begin{matrix}
Q_{\pi} = \ R(S,A) + \sum_{\begin{matrix}
S^{&#39;} \in S \\
\  \\
\end{matrix}}^{}{P_{T}\left( S^{&#39;} \middle| A,S \right)V_{\pi}\left(
S^{&#39;} \right)} \tag{2} \\
\end{matrix}\]</span></p>
<p>(2)式的第一项为在状态<span
class="math inline">\(s\)</span>采取动作<span
class="math inline">\(a\)</span>后的立即奖励，第二项为转移到状态<span
class="math inline">\(s^{&#39;}\)</span>后的未来预期奖励。由此，可对式（1）进行重写为如下格式</p>
<p><span class="math display">\[\begin{matrix}
V_{\pi}(S) = \sum_{A \in \mathcal{A}}^{}{\pi\left( A \middle| S
\right)Q_{\pi}(S,A)} \tag{3} \\
\end{matrix}\]</span></p>
<p><strong>定义4：</strong>
最优确定性策略表示为Q函数取最大值时的动作<span
class="math inline">\(a\)</span>，定义为如下公式：</p>
<p><span class="math display">\[\begin{matrix}
{\overline{\pi}}^{*}(S) = \underset{A \in
\mathcal{A}}{argmax}Q_{\pi}(S,A) \tag{4} \\
\end{matrix}\]</span></p>
<p>马尔科夫决策过程可以映射为agent在一个grid
world中通过强化学习来抵达目的地，其目标是：学习一个最优策略，找到当前状态下采取哪个动作能够获得最大的奖励值，以此类推形成一组最优动作序列。当该动作有助于抵达目标状态则指定一个正向的奖励值，否则是负向的惩罚值，最后确保累计得到的奖励值是最大化的。但是，当agent在完成比较复杂的任务时，这个过程中的奖励函数是很难指定，例如行人的出行决策，可能受到多种因素的影响，无法显示给定，因此我们希望找到一种方法可以推导从观测到的行人出行序列中推到出这个奖励函数，由此引出逆向强化学习。</p>
<ol start="3" type="1">
<li><strong>逆向强化学习 Inverse Reinforcement
Learning（IRL）</strong></li>
</ol>
<p>逆向强化学习假设输入元组为<span
class="math inline">\(MDP/R\)</span>，也就是<span
class="math inline">\(MDP\)</span>中没有奖励函数<span
class="math inline">\(R\)</span>。相反我们拥有一系列观测轨迹（序列、行为）：<span
class="math inline">\(\mathcal{B =
\{}B_{1},B_{2},\ldots,B_{n}\}\)</span>，其中<span
class="math inline">\(\mathcal{B \ni}B = \left( \left( S_{1},A_{1}
\right),\ldots,\left( S_{mB},A_{mB} \right) \right) \in (S \times
A)^{mB}\)</span>。逆向强化学习的目的在于通过已知观测轨迹，学习到激励agent产生如此轨迹的奖励函数。</p>
<p><strong>特征匹配IRL算法</strong></p>
<p><span class="math inline">\(f_{s,a}:S \times
A\)</span>为给定观测轨迹元组的特征函数,通过多条已知观测轨迹，我们可以计算出经验特征值<span
class="math inline">\(f_{\mathcal{B}} = \frac{1}{\left| \mathcal{B}
\right|}\sum_{B\mathcal{\in B}}^{}f_{B}\)</span>，其中<span
class="math inline">\(f_{B} = \frac{1}{|B|}\sum_{(S \times A) \in
B}^{}f_{S,A}\)</span>。假设IRL学得了一个奖励函数<span
class="math inline">\(\widehat{R}\)</span>，同时RL算法通过<span
class="math inline">\(MDP/R \cup \widehat{R}\)</span>学到了策略函数<span
class="math inline">\(\widehat{\pi}\)</span>，那么我们可以假设IRL满足以下约束：</p>
<p><span class="math display">\[\begin{matrix}
E\left\lbrack f_{s,a} \middle| \widehat{\pi} \right\rbrack =
f_{\mathcal{B}} \tag{5} \\
\end{matrix}\]</span></p>
<p>因此，特征匹配IRL算法（学徒算法、最大边际效应算法等）会通过以下两步进行学习直至收敛：</p>
<ol type="1">
<li><p>利用RL算法，通过当前奖励函数<span
class="math inline">\(\widehat{R}\)</span>学习到策略函数<span
class="math inline">\(\widehat{\pi}\)</span></p></li>
<li><p>通过衡量当前策略函数<span
class="math inline">\(\widehat{\pi}\)</span>生成的轨迹与已知轨迹的损失值来更新奖励函数<span
class="math inline">\(\widehat{R}\)</span></p></li>
</ol>
<p><strong>最大因果熵逆向强化学习 Maximum Causal Entropy Inverse
Reinforcement Learning(MaxCausalEnt IRL)</strong></p>
<p>特征匹配的IRL算法是病态的算法，对于同一系列轨迹可能存在多种不同可解释的奖励函数，为了解决奖励函数与观测轨迹之间的模糊性，MaxCausalEnt通过引入最大熵原理来解决这一问题。举例来说，对于观测轨迹中没有出现过的状态，MaxCausalEnt
IRL算法对于未知动作的分配应当是等可能的。</p>
<p>对于因果熵，我们首先考虑因果策略模型 <span
class="math inline">\(\pi(A_{t}|S_{1:t},\mathcal{A}_{1:t - 1})\)</span>
和标准策略模型 <span
class="math inline">\(\pi(A_{t}|S_{t})\)</span>,区别在于因果策略模型认为某一动作的选择与之前所有的动作和状态均有关而标准策略模型认为只和当前状态有关。然而，尽管因果假设更加强大，在IRL中完全应用因果条件会破坏Markovian假设，从而导致求解问题更加复杂。综上，我们可以将继承因果模型的思想，提出如下约束:</p>
<p><span class="math display">\[\begin{matrix}\pi\left( A_{t} \middle|
S_{1:t},\mathcal{A}_{1:t - 1} \right) = \pi\left( A_{t} \middle| S_{t}
\right) \tag{6} \\
\end{matrix}\]</span></p>
<p>接下来对该问题的基础参数进行建模：</p>
<p><span class="math display">\[\begin{matrix}
R_{\theta}(S,A) = \theta^{T}f_{S,A} \tag{7} \\
\end{matrix}\]</span></p>
<p><span class="math display">\[\begin{matrix}
V_{\pi\theta}^{soft}\left( S_{t} \right) = log\sum_{A_{t}\mathcal{\in
A}}^{}e^{Q_{\pi\theta}^{soft}\left( A_{t},S_{t} \right)} \tag{8} \\
\end{matrix}\]</span></p>
<p><span class="math display">\[\begin{matrix}
Q_{\pi\theta}^{soft}\left( A_{t},S_{t} \right) = R_{\theta}\left(
S_{t},A_{t} \right) + \sum_{S^{&#39;} \in
S}^{}{P_{T}(S^{&#39;}|A_{t},S_{t})V_{\pi\theta}^{soft}\left( S^{&#39;}
\right)} \tag{9} \\
\end{matrix}\]</span></p>
<p><span class="math display">\[\begin{matrix}
\pi_{\theta}\left( A_{t} \middle| S_{t} \right) =
e^{Q_{\pi\theta}^{soft}\left( A_{t},S_{t} \right) -
V_{\pi\theta}^{soft}\left( S^{&#39;} \right)} \tag{10} \\
\end{matrix}\]</span></p>
<p><span class="math display">\[\begin{matrix}
\mu(\pi) = E\left\lbrack \sum_{t =
0}^{\infty}{\gamma^{&#39;}f_{S_{t},A_{t}}|\pi} \right\rbrack \tag{11} \\
\end{matrix}\]</span></p>
<p>当给定m条观测轨迹时，依据式（11）观测轨迹的特征期望为：</p>
<p><span class="math display">\[\begin{matrix}
\widehat{\mu_{E}} = \frac{1}{m}\sum_{i = 1}^{m}{\sum_{t =
0}^{\infty}{\gamma^{&#39;}f_{S_{t},A_{t}}^{i}}} \tag{12} \\
\end{matrix}\]</span></p>
<p>从概率的角度理解该问题，可认为存在一个潜在的概率分布，在该分布下产生了观测专家轨迹，结合最大熵原理，可以写出如下的约束模型：</p>
<p><span class="math display">\[\max{\  - plogp}\]</span></p>
<p><span class="math display">\[s.t.\ \sum_{Path\ B}^{}{P(B)\mu(B) =
\widehat{\mu_{E}}}\]</span></p>
<p><span class="math display">\[\begin{matrix}
\sum_{}^{}P = 1 \tag{12} \\
\end{matrix}\]</span></p>
<p>利用拉格朗日法，该最优化问题可以转化为：</p>
<p><span class="math display">\[\begin{matrix}
\min L = \sum_{B}^{}{plogp} - \sum_{j = 1}^{n}{\theta_{j}\left( p\mu_{j}
- \widehat{\mu_{E}} \right) - \theta_{0}\left( \sum_{}^{}p - 1 \right)}
\tag{14} \\
\end{matrix}\]</span></p>
<p>其中，拉格朗日函数的参数<span
class="math inline">\(\lambda\)</span>恰好是奖励函数的参数<span
class="math inline">\(\theta\)</span>。此外对于MDP问题来说，这个问题是凸函数，因此其最优解可以通过梯度优化的方式得到，但利用最大似然法求解式（14）中的参数时，会遇到未知的配分函数<span
class="math inline">\(e^{(1 -
\lambda_{0})}\)</span>，因此改为次梯度的方法求解:</p>
<p><span class="math display">\[{ \nabla L_{Dual}(\theta) =
f_{\mathcal{B}} - E\left\lbrack f(S,A) \middle| {\widehat{\pi}}_{\theta}
\right\rbrack
}{\begin{matrix}
= f_{\mathcal{B}} - \sum_{(S,A)}^{}{P(S){\widehat{\pi}}_{\theta}\left( A
\middle| S \right)f_{S,A}} \\
\end{matrix}
}\begin{matrix}
= f_{\mathcal{B}} - \sum_{(S,A)}^{}{D_{S}{\widehat{\pi}}_{\theta}\left(
A \middle| S \right)f_{S,A}} (16)  \\
\end{matrix}\]</span></p>
<p>次梯度为观测轨迹与生成轨迹的经验特征期望的差值,其中<span
class="math inline">\(D_{S}\)</span>为状态<span
class="math inline">\(S\)</span>的访问频次，以此来替代状态访问频率<span
class="math inline">\(P(S)\)</span>。同过式（16）我们可以知道，为了计算次梯度，我们首先需要计算两个变量<span
class="math inline">\(D_{S}\)</span>和<span
class="math inline">\({\widehat{\pi}}_{\theta}\)</span>，因此MaxCausalEnt
IRL算法的顶层设计伪代码如下：</p>
<img src="/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image1.png" class="" title="顶层设计伪代码">
<center>
<p>
Figure1 顶层设计伪代码
</p>
</center>
<ol type="1">
<li><strong>最大熵逆向强化学习算法框架</strong></li>
</ol>
<p>最大熵逆向强化学习的输入为：</p>
<ol type="1">
<li><p><span class="math inline">\(MDP/R\)</span>（状态空间<span
class="math inline">\(S\)</span>，动作空间<span
class="math inline">\(A\)</span>，状态转移概率<span
class="math inline">\(P_{T}\)</span>,最初状态分配<span
class="math inline">\(P_{0}\)</span>,最终状态集合<span
class="math inline">\(S_{\phi}\)</span>）</p></li>
<li><p><span
class="math inline">\(\mathcal{B}\)</span>(可观测轨迹、序列)</p></li>
<li><p><span class="math inline">\(\mathcal{F =}\forall_{(S,A) \in S
\times A}f_{S,A}\)</span>(状态动作对的特征函数)</p></li>
</ol>
<p>首先我们要进行梯度下降的计算，为了标记的简化，我们对式（16）进行如下简化定义：</p>
<p><span class="math display">\[\begin{matrix}
\nabla_{\theta}F(S,A) = D_{S}{\widehat{\pi}}_{\theta}\left( A \middle| S
\right)f_{S,A} \tag{17} \\
\end{matrix}\]</span></p>
<p>我们接下来定义梯度下降算法作为算法1的详细版本，其伪代码版本如下：</p>
<img src="/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image2.png" class="" title="梯度下降算法">
<center>
<p>
Figure2 梯度下降算法
</p>
</center>
<p>在这个算法中，我们引入了两个超参数：</p>
<ol type="1">
<li><p><span
class="math inline">\(\lambda\)</span>-学习速率，通过交叉验证选择</p></li>
<li><p><span
class="math inline">\(t\)</span>-学习速率衰减，算法第10行说明随着迭代次数的增加，学习速率不断衰减，可以使得算法更易收敛</p></li>
</ol>
<p>在算法4-10行中，我们通过每一状态和动作对的加和来计算梯度，如果状态和动作空间不离散，或者空间过大难以迭代时，我们可以采取小批量随机梯度下降的算法，虽然每次迭代收敛会变慢，但整体算法时间会缩短。</p>
<p>在算法中，我们并不知道<span
class="math inline">\(\nabla_{\theta}F(S,A)\)</span>是如何计算的，其中涉及到<span
class="math inline">\(D\)</span>和<span
class="math inline">\({\widehat{\pi}}_{\theta}\)</span>两个参数，在式（8）（9）（10）中可以看出，如果我们知道了<span
class="math inline">\(V_{ {\widehat{\pi}
}_{\theta}}^{soft}\)</span>，我们就可以计算<span
class="math inline">\({\widehat{\pi}}_{\theta}\)</span>，接下来我们介绍计算状态值函数<span
class="math inline">\(V_{ {\widehat{\pi} }_{\theta}
}^{soft}\)</span>的算法3，在此之前首先给出softmax的定义公式：</p>
<p><span class="math display">\[\begin{matrix}
soft\max(x,y) = \log\left( e^{x} + e^{y} \right) = \max(x,y) +
\log\left( 1 + e^{\max(x,y) - \min(x,y)} \right) \tag{18} \\
\end{matrix}\]</span></p>
<img src="/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image3.png" class="" title="状态值函数计算">
<center>
<p>
Figure3 状态值函数计算
</p>
</center>
<p>在上述算法中，我们引入了一个新的超参数<span
class="math inline">\(\phi\)</span>-表示种植状态的奖励函数，对于拥有一系列<span
class="math inline">\(S_{\phi}\)</span>,<span
class="math inline">\(\phi\)</span>可以被定义为如下公式：</p>
<p><span class="math display">\[\begin{matrix}
\phi(S) = \left\{ \begin{matrix}
- \infty,\ \ \ \ \ \ \ \ if\ S \notin \mathcal{S}_{\phi} \\
0,\ \ \ \ \ \ \ \ \ othterwise\  \\
\end{matrix} \right.\  \tag{19} \\
\end{matrix}\]</span></p>
<p>结合式（19）和算法8-12行，我们对该算法核心关键要素进行总结，即从最终状态开始迭代，不断为每个状态进行赋值，直至算法收敛。由于非最终状态的初始值设置为<span
class="math inline">\(-
\infty\)</span>，因此在每一次迭代中，只有通过潜在动作与有值的状态相接的状态才会更新状态值，通过不断迭代，寻找每一状态的值函数，当值函数的变化小于临界值时，算法结束。</p>
<p>在算法三结束后，我们通过式子（8）-（10）的计算，可以得到<span
class="math inline">\({\widehat{\pi}}_{\theta}\)</span>的计算值，为了计算式（17），我们还需要通过<span
class="math inline">\({\widehat{\pi}}_{\theta}\)</span>计算状态访问频次，由此引出算法4，其伪代码如下：</p>
<img src="/2022/01/15/%E6%9C%80%E5%A4%A7%E5%9B%A0%E6%9E%9C%E7%86%B5%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/image4.png" class="" title="状态访问频次计算">
<center>
<p>
Figure4 状态访问频次计算
</p>
</center>
<ol start="5" type="1">
<li><strong>最大熵深度逆向强化学习</strong></li>
</ol>
<p>在算法2-4中，我们利用全部状态和动作进行循环，求得<span
class="math inline">\(\nabla
F,D_{S},V(S)\)</span>等参数值，从这样的算法流程我们可以看出传统IRL在表征能力上具有一定的局限性，只能够应用在规模小、离散的任务上，而深度神经网络可以针对这个问题进行很好的改进。由于深度神经网络对高层非线性函数具有较好的表征能力，利用深度神经网络近似回报函数，可以通过神经元分层结构中的许多非线性结果的组合，使得其更贴近真实的奖励函数，从而提高IRL的非线性表征能力，同时深度神经网络对于大规模数据具有良好的算法处理能力，解决了IRL只能应用在小规模、离散任务的缺点。</p>
<ol start="6" type="1">
<li><strong>总结</strong></li>
</ol>
<p>最大熵逆向强化学习是建模序列化行为的重要工具，以马尔可夫决策过程为基准同时通过观测轨迹（序列、行为）学习奖励函数，进而对未来的轨迹进行预测，同时引入最大熵原理以解决多奖励函数均可解释可观测轨迹的两义性问题。</p>
<p><strong>参考文献</strong></p>
<p>[1] Heim E. A Practitioner's Guide to Maximum Causal Entropy Inverse
Reinforcement Learning, Starting from Markov Decision
Processes[Z].CARNEGIE-MELLON UNIV PITTSBURGH PA PITTSBURGH United
States, 2019.</p>
<p>[2] 陈希亮, 曹雷, 何明, 等.
深度逆向强化学习研究综述[J].计算机工程与应用, 2018,54(05):24-35.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>只想买点薯条</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="李博洋 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.png" alt="李博洋 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 逆向强化学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
            </div>
            <div class="post-nav-item">
                <a href="/2022/02/15/DeepIRL/" rel="next" title="深度逆向强化学习公式推导">
                  深度逆向强化学习公式推导 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李博洋</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.js","integrity":"sha256-hlC2uSQYTmPsrzGZTEQEg9PZ1a/+SV6VBCTclohf2og="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
